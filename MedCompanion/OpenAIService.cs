using System;
using System.Collections.Generic;
using System.Linq;
using System.Net.Http;
using System.Text;
using System.Text.Json;
using System.Threading;
using System.Threading.Tasks;
using MedCompanion.Models;
using MedCompanion.Services;
using MedCompanion.Services.LLM;

namespace MedCompanion
{
    public class OpenAIService
    {
        private readonly LLMServiceFactory _llmFactory;
        private AppSettings _settings; // ‚ö†Ô∏è NON readonly pour permettre ReloadSettings()
        private readonly PromptConfigService _promptConfig;
        private readonly AnonymizationService _anonymizationService;
        private readonly PromptTrackerService? _promptTracker;  // ‚úÖ NOUVEAU - Tracking des prompts

        // Cache des prompts pour √©viter les appels r√©p√©t√©s
        private string _cachedSystemPrompt;
        private string _cachedNoteStructurationPrompt;
        private string _cachedChatInteractionPrompt;

        public OpenAIService(LLMServiceFactory llmFactory, PromptConfigService promptConfig, AnonymizationService anonymizationService, PromptTrackerService? promptTracker = null)
        {
            _llmFactory = llmFactory;
            _settings = AppSettings.Load();
            _promptConfig = promptConfig; // ‚úÖ Utiliser l'instance partag√©e
            _anonymizationService = anonymizationService;
            _promptTracker = promptTracker;  // ‚úÖ NOUVEAU - Stocker le tracker

            // Charger les prompts initialement
            LoadPrompts();

            // S'abonner √† l'√©v√©nement de rechargement des prompts (si _promptConfig n'est pas null)
            if (_promptConfig != null)
            {
                _promptConfig.PromptsReloaded += OnPromptsReloaded;
            }
        }
        
        /// <summary>
        /// Charge les prompts depuis le service de configuration
        /// </summary>
        private void LoadPrompts()
        {
            // ‚úÖ V√©rifier si _promptConfig est null
            if (_promptConfig == null)
            {
                _cachedSystemPrompt = "";
                _cachedNoteStructurationPrompt = "";
                _cachedChatInteractionPrompt = "";
                System.Diagnostics.Debug.WriteLine("[OpenAIService] Prompts non charg√©s (PromptConfig null)");
                return;
            }

            _cachedSystemPrompt = _promptConfig.GetActivePrompt("system_global");
            _cachedNoteStructurationPrompt = _promptConfig.GetActivePrompt("note_structuration");
            _cachedChatInteractionPrompt = _promptConfig.GetActivePrompt("chat_interaction");

            System.Diagnostics.Debug.WriteLine("[OpenAIService] Prompts charg√©s depuis la configuration");
        }
        
        /// <summary>
        /// Gestionnaire d'√©v√©nement pour le rechargement des prompts
        /// </summary>
        private void OnPromptsReloaded(object? sender, EventArgs e)
        {
            LoadPrompts();
            System.Diagnostics.Debug.WriteLine("[OpenAIService] ‚úÖ Prompts recharg√©s automatiquement suite √† une modification");
        }

        /// <summary>
        /// Recharge les settings depuis le fichier de configuration (apr√®s modification des param√®tres)
        /// ‚úÖ IMPORTANT: √Ä appeler apr√®s toute sauvegarde des param√®tres dans ParametresDialog
        /// </summary>
        public void ReloadSettings()
        {
            _settings = AppSettings.Load();
            System.Diagnostics.Debug.WriteLine($"[OpenAIService] ‚úÖ Settings recharg√©s - AnonymizationModel: {_settings.AnonymizationModel}");
        }

        /// <summary>
        /// R√©cup√®re le provider LLM actuellement actif (permet le changement dynamique)
        /// </summary>
        private ILLMService GetCurrentLLM()
        {
            return _llmFactory.GetCurrentProvider();
        }

        private string BuildSystemPrompt()
        {
            var medecin = _settings.Medecin;
            var ville = _settings.Ville;
            
            // Utiliser le prompt en cache (recharg√© automatiquement via √©v√©nement)
            var prompt = _cachedSystemPrompt
                .Replace("{{Medecin}}", medecin)
                .Replace("{{Ville}}", ville);

            return prompt;
        }

        public bool IsApiKeyConfigured()
        {
            // D√©l√©guer la v√©rification au LLM service actuel
            return GetCurrentLLM().IsConfigured();
        }

        public async Task<(bool success, string result, double relevanceWeight)> StructurerNoteAsync(
            string nomComplet,
            string sexe,
            string noteBrute,
            CancellationToken cancellationToken = default)
        {
            if (!IsApiKeyConfigured())
            {
                return (false, "LLM non configur√©. V√©rifiez la configuration dans la barre LLM.", 0.0);
            }

            if (string.IsNullOrWhiteSpace(nomComplet) || string.IsNullOrWhiteSpace(noteBrute))
            {
                return (false, "Le nom complet et la note brute sont requis.", 0.0);
            }

            try
            {
                // V√©rifier l'annulation
                cancellationToken.ThrowIfCancellationRequested();

                // ‚úÖ √âTAPE 1 : Cr√©er m√©tadonn√©es pour l'anonymisation
                var parts = nomComplet.Split(' ', StringSplitOptions.RemoveEmptyEntries);
                var prenom = parts.Length > 0 ? parts[0] : "";
                var nom = parts.Length > 1 ? string.Join(" ", parts.Skip(1)) : "";
                
                // Si un seul mot, on le consid√®re comme Nom
                if (string.IsNullOrEmpty(nom) && !string.IsNullOrEmpty(prenom))
                {
                    nom = prenom;
                    prenom = "";
                }

                var patientMeta = new PatientMetadata 
                { 
                    Nom = nom, 
                    Prenom = prenom, 
                    Sexe = sexe 
                };

                // Anonymiser le nom du patient (attendre le r√©sultat async)
                var (nomAnonymise, anonContext) = await _anonymizationService.AnonymizeAsync(nomComplet, patientMeta);

                // V√©rifier si une date est d√©j√† pr√©sente dans la note brute
                var hasDate = System.Text.RegularExpressions.Regex.IsMatch(
                    noteBrute,
                    @"(date|entretien|consultation|rendez-vous|rdv).*\d{1,2}[\/\-\.]\d{1,2}[\/\-\.]\d{2,4}",
                    System.Text.RegularExpressions.RegexOptions.IgnoreCase
                );

                var dateInstruction = hasDate
                    ? ""
                    : $"\n\nIMPORTANT: Aucune date de consultation n'est mentionn√©e dans la note brute. Utilise automatiquement la date d'aujourd'hui ({DateTime.Now:dd/MM/yyyy}) comme date de l'entretien dans le compte-rendu.";

                // ‚úÖ √âTAPE 2 : Utiliser le pseudonyme dans le prompt
                var basePrompt = _cachedNoteStructurationPrompt
                    .Replace("{{Nom_Complet}}", nomAnonymise)  // ‚úÖ Pseudonyme au lieu du vrai nom
                    .Replace("{{Date_Instruction}}", dateInstruction)
                    .Replace("{{Note_Brute}}", noteBrute);

                // NOUVEAU : Ajouter √©valuation du poids de pertinence
                var userPrompt = basePrompt + @"

---
√âVALUATION IMPORTANCE (pour mise √† jour synth√®se patient) :

Apr√®s avoir structur√© la note, √©value son importance pour mettre √† jour la synth√®se patient (0.0 √† 1.0) :

**Poids 1.0 (critique)** : Nouveau diagnostic pos√©, changement majeur de traitement (initiation/arr√™t), √©v√©nement grave (hospitalisation, crise, TS), note fondatrice (premi√®re consultation)
**Poids 0.7-0.9 (tr√®s important)** : √âvolution significative des sympt√¥mes, ajustement posologique important, nouveau trouble associ√©
**Poids 0.4-0.6 (mod√©r√©)** : Note de suivi standard avec informations cliniques pertinentes
**Poids 0.1-0.3 (mineur)** : Suivi de routine sans nouvelle information majeure

√Ä la fin de ta r√©ponse, apr√®s le markdown de la note structur√©e, ajoute une ligne :
POIDS_SYNTHESE: X.X
";

                // ‚úÖ UTILISER LE MOD√àLE LOCAL OLLAMA pour la structuration (s√©curit√© des donn√©es)
                ILLMService structurationProvider = new OllamaLLMProvider(_settings.OllamaBaseUrl, _settings.AnonymizationModel);
                System.Diagnostics.Debug.WriteLine($"[OpenAIService] ‚úÖ Structuration note via mod√®le LOCAL Ollama : {_settings.AnonymizationModel}");

                var systemPrompt = BuildSystemPrompt();
                var messages = new List<(string role, string content)>
                {
                    ("user", userPrompt)
                };
                var (success, result, error) = await structurationProvider.ChatAsync(systemPrompt, messages);

                if (!success)
                {
                    return (false, error ?? "Erreur lors de la structuration.", 0.0);
                }

                // Extraire le poids de la r√©ponse
                double weight = ExtractWeightFromResponse(result ?? "");

                // Retirer la ligne POIDS_SYNTHESE du markdown
                string cleanedMarkdown = RemoveWeightLine(result ?? "");

                // ‚úÖ √âTAPE 3 : D√©sanonymiser le r√©sultat
                string deanonymizedMarkdown = _anonymizationService.Deanonymize(cleanedMarkdown, anonContext);

                // ‚úÖ √âTAPE 4 : Logger le prompt (si tracker disponible)
                if (_promptTracker != null)
                {
                    try
                    {
                        // R√©cup√©rer les infos du provider LLM actuel
                        var llmProvider = GetCurrentLLM();
                        var providerType = llmProvider.GetType().Name;
                        var providerName = providerType.Replace("LLMProvider", ""); // Ex: "OpenAI" ou "Ollama"

                        // D√©terminer le nom du mod√®le (simplifi√©)
                        string modelName = providerType.Contains("OpenAI") ? "GPT-4" : "Ollama";

                        _promptTracker.LogPrompt(new PromptLogEntry
                        {
                            Timestamp = DateTime.Now,
                            Module = "Note",  // ‚úÖ Correspond au filtre dans l'UI
                            SystemPrompt = systemPrompt,  // Prompt syst√®me (pas de donn√©es patient)
                            UserPrompt = userPrompt,      // ‚ö†Ô∏è Contient le PSEUDONYME (anonymis√©)
                            AIResponse = deanonymizedMarkdown,  // ‚úÖ R√©ponse D√âSANONYMIS√âE (vrai nom)
                            TokensUsed = 0,  // TODO: r√©cup√©rer depuis la r√©ponse LLM si disponible
                            LLMProvider = providerName,
                            ModelName = modelName,
                            Success = true,
                            Error = null
                        });
                    }
                    catch (Exception logEx)
                    {
                        // Ne pas bloquer la structuration si le logging √©choue
                        System.Diagnostics.Debug.WriteLine($"[OpenAI] Erreur logging prompt: {logEx.Message}");
                    }
                }

                return (true, deanonymizedMarkdown, weight);
            }
            catch (Exception ex)
            {
                return (false, $"Erreur inattendue: {ex.Message}", 0.0);
            }
        }

        /// <summary>
        /// Extrait le poids de pertinence depuis la r√©ponse de l'IA
        /// </summary>
        private double ExtractWeightFromResponse(string response)
        {
            try
            {
                var match = System.Text.RegularExpressions.Regex.Match(
                    response,
                    @"POIDS_SYNTHESE:\s*(\d+\.?\d*)",
                    System.Text.RegularExpressions.RegexOptions.IgnoreCase
                );

                if (match.Success && double.TryParse(match.Groups[1].Value.Replace(',', '.'),
                    System.Globalization.NumberStyles.Float,
                    System.Globalization.CultureInfo.InvariantCulture,
                    out double weight))
                {
                    return Math.Clamp(weight, 0.0, 1.0); // S√©curit√©: entre 0 et 1
                }
            }
            catch (Exception ex)
            {
                System.Diagnostics.Debug.WriteLine($"[OpenAI] Erreur extraction poids: {ex.Message}");
            }

            return 0.5; // Valeur par d√©faut si parsing √©choue
        }

        /// <summary>
        /// Retire la ligne POIDS_SYNTHESE du markdown
        /// </summary>
        private string RemoveWeightLine(string markdown)
        {
            try
            {
                return System.Text.RegularExpressions.Regex.Replace(
                    markdown,
                    @"POIDS_SYNTHESE:\s*\d+\.?\d*\s*",
                    "",
                    System.Text.RegularExpressions.RegexOptions.IgnoreCase
                ).Trim();
            }
            catch
            {
                return markdown;
            }
        }

        public async Task<(bool success, string result)> ChatAvecContexteAsync(
            string contexte,
            string question,
            List<ChatExchange>? historique = null,
            string? customSystemPrompt = null,
            string? compactedMemory = null,
            List<ChatExchange>? recentSavedExchanges = null,
            int maxTokens = 1500)
        {
            if (!IsApiKeyConfigured())
            {
                return (false, "LLM non configur√©. V√©rifiez la configuration dans la barre LLM.");
            }

            if (string.IsNullOrWhiteSpace(question))
            {
                return (false, "La question est requise.");
            }

            try
            {
                // Construire le prompt syst√®me
                string systemPrompt;
                if (customSystemPrompt != null)
                {
                    // Un prompt personnalis√© est fourni (ex: attestations, lettres, etc.)
                    systemPrompt = customSystemPrompt;
                }
                else
                {
                    // Mode chat interactif : combiner system_global + chat_interaction
                    var baseSystemPrompt = BuildSystemPrompt();
                    
                    // Ajouter les instructions sp√©cifiques au chat
                    if (!string.IsNullOrEmpty(_cachedChatInteractionPrompt))
                    {
                        systemPrompt = baseSystemPrompt + "\n\n" + _cachedChatInteractionPrompt;
                    }
                    else
                    {
                        systemPrompt = baseSystemPrompt;
                    }
                }

                // Construire le userPrompt avec contexte et m√©moire intelligente
                var userPromptBuilder = new StringBuilder();

                // 1. Ajouter le contexte patient (TOUJOURS EN PREMIER)
                if (!string.IsNullOrWhiteSpace(contexte))
                {
                    userPromptBuilder.AppendLine("CONTEXTE PATIENT");
                    userPromptBuilder.AppendLine("================");
                    userPromptBuilder.AppendLine(contexte);
                    userPromptBuilder.AppendLine();
                }

                // 2. Ajouter la m√©moire compact√©e (r√©sum√© des anciens √©changes)
                if (!string.IsNullOrWhiteSpace(compactedMemory))
                {
                    userPromptBuilder.AppendLine("M√âMOIRE COMPACT√âE (anciens √©changes)");
                    userPromptBuilder.AppendLine("====================================");
                    userPromptBuilder.AppendLine(compactedMemory);
                    userPromptBuilder.AppendLine();
                }

                // 3. Ajouter les √©changes r√©cents sauvegard√©s (10 derniers)
                if (recentSavedExchanges != null && recentSavedExchanges.Count > 0)
                {
                    userPromptBuilder.AppendLine("√âCHANGES R√âCENTS SAUVEGARD√âS");
                    userPromptBuilder.AppendLine("============================");
                    foreach (var exchange in recentSavedExchanges)
                    {
                        userPromptBuilder.AppendLine($"[{exchange.Timestamp:dd/MM/yyyy HH:mm}] {exchange.Etiquette}");
                        userPromptBuilder.AppendLine($"Q: {exchange.Question}");
                        userPromptBuilder.AppendLine($"R: {exchange.Response}");
                        userPromptBuilder.AppendLine();
                    }
                }

                // 4. Ajouter l'historique temporaire des 3 derniers √©changes (session en cours)
                if (historique != null && historique.Count > 0)
                {
                    var recentHistory = historique.TakeLast(3);
                    userPromptBuilder.AppendLine("HISTORIQUE DE LA SESSION EN COURS");
                    userPromptBuilder.AppendLine("==================================");
                    foreach (var exchange in recentHistory)
                    {
                        userPromptBuilder.AppendLine($"Q: {exchange.Question}");
                        userPromptBuilder.AppendLine($"R: {exchange.Response}");
                        userPromptBuilder.AppendLine();
                    }
                }

                // 5. Ajouter la question actuelle
                userPromptBuilder.AppendLine("QUESTION ACTUELLE");
                userPromptBuilder.AppendLine("=================");
                userPromptBuilder.AppendLine(question);

                var userPrompt = userPromptBuilder.ToString();

                // Utiliser le LLM service unifi√© (provider actuel)
                var currentLLM = GetCurrentLLM();
                var llmName = currentLLM.GetType().Name;
                var modelName = currentLLM.GetModelName();

                // LOG D√âTAILL√â POUR DEBUG
                System.Diagnostics.Debug.WriteLine("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê");
                System.Diagnostics.Debug.WriteLine("‚îÇ [OpenAIService] ChatAvecContexteAsync - Appel LLM          ‚îÇ");
                System.Diagnostics.Debug.WriteLine("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò");
                System.Diagnostics.Debug.WriteLine($"ü§ñ Provider: {llmName}");
                System.Diagnostics.Debug.WriteLine($"üè∑Ô∏è  Mod√®le: {modelName}");
                System.Diagnostics.Debug.WriteLine($"üìä SystemPrompt: {systemPrompt?.Length ?? 0} caract√®res");
                System.Diagnostics.Debug.WriteLine($"üìù UserPrompt: {userPrompt?.Length ?? 0} caract√®res");
                System.Diagnostics.Debug.WriteLine("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ");

                var messages = new List<(string role, string content)>
                {
                    ("user", userPrompt)
                };
                var (success, result, error) = await currentLLM.ChatAsync(systemPrompt, messages, maxTokens);

                System.Diagnostics.Debug.WriteLine($"‚úÖ Succ√®s: {success}");
                System.Diagnostics.Debug.WriteLine($"üì§ R√©sultat: {result?.Length ?? 0} caract√®res");
                System.Diagnostics.Debug.WriteLine($"‚ùå Erreur: {error ?? "Aucune"}");
                System.Diagnostics.Debug.WriteLine("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò");

                return (success, result ?? error ?? "Aucun contenu retourn√©.");
            }
            catch (Exception ex)
            {
                return (false, $"Erreur inattendue: {ex.Message}");
            }
        }

        /// <summary>
        /// G√©n√®re du texte √† partir d'un prompt personnalis√© (pour synth√®ses, etc.)
        /// </summary>
        public async Task<(bool success, string result, string? error)> GenerateTextAsync(string prompt, int maxTokens = 3000)
        {
            if (!IsApiKeyConfigured())
            {
                return (false, "", "LLM non configur√©. V√©rifiez la configuration dans la barre LLM.");
            }

            if (string.IsNullOrWhiteSpace(prompt))
            {
                return (false, "", "Le prompt est requis.");
            }

            try
            {
                var systemPrompt = BuildSystemPrompt();
                
                // Utiliser le LLM service unifi√© (provider actuel)
                var messages = new List<(string role, string content)>
                {
                    ("user", prompt)
                };
                var (success, result, error) = await GetCurrentLLM().ChatAsync(systemPrompt, messages);

                if (success)
                {
                    return (true, result ?? "", null);
                }
                else
                {
                    return (false, "", error ?? result ?? "Erreur inconnue");
                }
            }
            catch (Exception ex)
            {
                return (false, "", $"Erreur inattendue: {ex.Message}");
            }
        }
        /// <summary>
        /// Extrait les entit√©s sensibles (PII) d'un texte via le LLM actif.
        /// </summary>
        public async Task<PIIExtractionResult> ExtractPIIAsync(string text)
        {
            if (string.IsNullOrWhiteSpace(text))
                return new PIIExtractionResult();

            if (!IsApiKeyConfigured())
            {
                System.Diagnostics.Debug.WriteLine("[OpenAIService] LLM non configur√© pour l'extraction PII");
                return new PIIExtractionResult();
            }

            try
            {
                var textToAnalyze = text.Length > 2000 ? text.Substring(0, 2000) : text;

                var prompt = $@"Tu es un expert en confidentialit√© des donn√©es m√©dicales.
Ta t√¢che est d'analyser le texte OCR suivant pour identifier TOUTES les informations identifiantes (PII).

Texte √† analyser :
""{textToAnalyze}""

Instructions :
1. Extrais TOUS les noms de personnes (Patient, M√©decin, Proche).
2. Extrais TOUTES les dates (Naissance, Consultation, Courrier).
3. Extrais TOUS les lieux (Villes, Adresses pr√©cises, Cliniques).
4. Extrais TOUTES les organisations (H√¥pitaux, Laboratoires).
5. R√©ponds UNIQUEMENT au format JSON strict. Pas de markdown, pas d'explications.

Format JSON attendu :
{{
    ""noms"": [""M. Dupont"", ""Dr. House""],
    ""dates"": [""12/05/2022"", ""10 janvier 2023""],
    ""lieux"": [""Paris"", ""10 rue de la Paix""],
    ""organisations"": [""Clinique des Lilas""]
}}";

                var systemPrompt = "Tu es un extracteur d'entit√©s JSON strict. Tu ne r√©ponds jamais autre chose que du JSON.";
                
                var messages = new List<(string role, string content)>
                {
                    ("user", prompt)
                };

                // ‚úÖ S√âCURIT√â : L'extraction PII doit TOUJOURS utiliser un mod√®le Ollama local
                // pour √©viter d'envoyer des donn√©es sensibles au cloud AVANT anonymisation
                if (string.IsNullOrEmpty(_settings.AnonymizationModel))
                {
                    System.Diagnostics.Debug.WriteLine("[OpenAIService] ‚ùå ERREUR : Mod√®le d'anonymisation non configur√©");
                    throw new InvalidOperationException(
                        "‚ùå S√âCURIT√â : Mod√®le d'anonymisation local (Ollama) non configur√©.\n\n" +
                        "L'extraction PII ne peut PAS utiliser OpenAI (cloud) pour des raisons de confidentialit√©.\n" +
                        "Les donn√©es sensibles du patient doivent rester locales.\n\n" +
                        "Solution : Configurez 'AnonymizationModel' dans Param√®tres > Anonymisation.\n" +
                        "Exemple : llama3.2, mistral, phi3"
                    );
                }

                // Cr√©er un provider Ollama avec le mod√®le d√©di√© (toujours local)
                ILLMService piiProvider = new OllamaLLMProvider(_settings.OllamaBaseUrl, _settings.AnonymizationModel);
                System.Diagnostics.Debug.WriteLine($"[OpenAIService] ‚úÖ Extraction PII via mod√®le LOCAL Ollama : {_settings.AnonymizationModel}");

                var (success, result, error) = await piiProvider.ChatAsync(systemPrompt, messages);

                if (!success || string.IsNullOrWhiteSpace(result))
                {
                    System.Diagnostics.Debug.WriteLine($"[OpenAIService] Erreur extraction PII : {error}");
                    return new PIIExtractionResult();
                }

                var json = result.Trim();
                if (json.StartsWith("```json")) json = json.Replace("```json", "").Replace("```", "");
                if (json.StartsWith("```")) json = json.Replace("```", "");
                
                var options = new JsonSerializerOptions { PropertyNameCaseInsensitive = true };
                var extraction = JsonSerializer.Deserialize<PIIExtractionResult>(json, options);

                return extraction ?? new PIIExtractionResult();
            }
            catch (Exception ex)
            {
                System.Diagnostics.Debug.WriteLine($"[OpenAIService] Exception lors de l'extraction PII : {ex.Message}");
                return new PIIExtractionResult();
            }
        }
    }
}
